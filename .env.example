# ============================================================================
# PERSONALITY SELECTION
# ============================================================================
# Choose which AI personality to activate. Each personality has:
#   - Unique wake word phrase for activation
#   - Custom voice, speed, and speaking style
#   - Personality-specific system prompt and knowledge base
#   - Pre-recorded filler phrases for low-latency responses
#
# Available personalities:
#   - johnny: Laid-back tiki bartender with cocktail expertise
#   - mr_lincoln: Abraham Lincoln, 16th President (Disney animatronic homage)
#   - leopold: Eccentric conspiracy theorist with wild backstory
#
# To create new personalities, see: personalities/README.md
PERSONALITY=johnny

# ============================================================================
# OPENAI API CONFIGURATION
# ============================================================================
# Your OpenAI API key for Whisper (speech-to-text), GPT (conversation),
# and TTS (text-to-speech) services.
#
# Get your API key from: https://platform.openai.com/api-keys
# Usage costs (approximate per conversation turn):
#   - Whisper: $0.006 per minute of audio
#   - GPT-4o-mini: $0.001-0.005 per turn
#   - TTS: $0.015 per 1000 characters
OPENAI_API_KEY=your_openai_api_key_here

# ============================================================================
# WAKE WORD DETECTION (OpenWakeWord)
# ============================================================================
# Wake word detection runs continuously in the background, listening for
# the personality's activation phrase (e.g., "Hey, Johnny").
#
# This system uses OpenWakeWord - a free, open-source wake word engine that
# runs entirely locally with no API key required. Each personality includes
# a trained wake word model (.onnx file) in its directory.
#
# To train custom wake words: docs/TRAIN_WAKE_WORDS.md

# ============================================================================
# AUDIO DEVICE CONFIGURATION
# ============================================================================
# Audio devices for microphone input and speaker/animatronic output.
#
# To list available devices, run:
#   python -m jf_sebastian.modules.audio_output
#
# Device names use case-insensitive partial matching. For example:
#   "MacBook" will match "MacBook Air Microphone"
#   "Arsvita" will match "Arsvita Car Audio Bluetooth"
#
# For Teddy Ruxpin, OUTPUT_DEVICE_NAME should be your Bluetooth cassette
# adapter (e.g., Arsvita). The system outputs stereo audio:
#   LEFT channel  = Voice audio (plays through Teddy's speaker)
#   RIGHT channel = PPM control signals (drives motors for mouth/eyes)
INPUT_DEVICE_NAME=MacBook Air Microphone
OUTPUT_DEVICE_NAME=Arsvita

# Output device type - determines audio processing pipeline
# Options:
#   - teddy_ruxpin: Stereo with PPM control signals (LEFT=voice, RIGHT=PPM)
#   - squawkers_mccaw: Simple stereo audio (LEFT=voice, RIGHT=voice)
#
# This setting controls which output device is used for audio generation.
# Each device type has different capabilities:
#   - Teddy Ruxpin includes full animatronic control (mouth, eyes)
#   - Squawkers McCaw plays audio without PPM motor control
OUTPUT_DEVICE_TYPE=teddy_ruxpin

# ============================================================================
# AUDIO PROCESSING CONFIGURATION
# ============================================================================
# Sample rate for audio capture and processing (Hz)
# Valid values: 16000, 22050, 44100, or 48000
#
# Recommendations:
#   - 16000 Hz: Best for wake word detection (lower CPU usage)
#   - 44100 Hz: CD-quality audio, better for music/high-fidelity
#
# Note: Final output is always resampled to 44100 Hz for precise PPM timing
SAMPLE_RATE=16000

# Audio chunk size for real-time processing (samples)
# Smaller = lower latency but higher CPU usage
# Larger = higher latency but lower CPU usage
#
# Default 1024 provides good balance at 16kHz (64ms chunks)
CHUNK_SIZE=1024

# ============================================================================
# VOICE ACTIVITY DETECTION (VAD)
# ============================================================================
# VAD distinguishes speech from background noise and silence.
# Uses WebRTC VAD algorithm to detect when you start/stop talking.

# VAD aggressiveness level (0-3)
# Controls how aggressive the VAD is at filtering out non-speech
#
#   0 = Least aggressive (picks up more ambient sound as speech)
#   1 = Low aggressiveness
#   2 = Medium aggressiveness
#   3 = Most aggressive (only clear speech triggers detection)
#
# Higher values reduce false triggers from background noise but may
# miss soft-spoken words. Lower values are more sensitive but may
# trigger on ambient sounds.
VAD_AGGRESSIVENESS=3

# Maximum silence duration before returning to IDLE state (seconds)
# If you stop talking for this long, the system assumes you're done
# and returns to listening for the wake word.
#
# Increase if you need long pauses while speaking.
# Decrease for faster timeout in noisy environments.
SILENCE_TIMEOUT=5.0

# Silence duration required to end speech detection (seconds)
# After you start talking, this much continuous silence indicates
# you've finished your sentence.
#
# Too low = May cut off your sentence if you pause briefly
# Too high = Adds delay before processing starts
#
# 1.0s is a natural pause length that works well for most speakers
SPEECH_END_SILENCE_SECONDS=1.0

# Minimum time to listen after wake word detection (seconds)
# Prevents the system from ending speech detection too quickly
# if there's a brief pause right after the wake word.
#
# Example: "Hey Johnny... [pause] ... what's in a Mai Tai?"
# This setting ensures the pause doesn't prematurely end listening.
MIN_LISTEN_SECONDS=1.0

# ============================================================================
# CONVERSATION SETTINGS
# ============================================================================
# Timeout to clear conversation history (seconds)
# If no interaction occurs for this duration, the conversation context
# is cleared and the system starts fresh.
#
# This prevents the AI from remembering stale context from old conversations
# while allowing natural back-and-forth within a session.
CONVERSATION_TIMEOUT=120.0

# Maximum conversation history to maintain (message count)
# Limits how many previous messages are sent to GPT for context.
#
# Higher = More context, better conversation coherence, higher API costs
# Lower = Less context, may forget recent topics, lower API costs
#
# Each message pair (user + assistant) counts as 2 messages.
# 20 messages = ~10 conversation turns of context.
MAX_HISTORY_LENGTH=20

# Minimum chunk length for streaming responses (word count)
# When streaming LLM responses, text is chunked by complete sentences.
# Each chunk will contain the minimum number of sentences that exceeds
# this word count threshold, enabling efficient TTS processing.
#
# Lower = More frequent, smaller chunks (faster initial response)
# Higher = Fewer, larger chunks (more efficient TTS batching)
#
# Default 15 words provides good balance for natural-sounding speech
# while maintaining low latency for the first chunk.
MIN_CHUNK_WORDS=15

# Maximum tokens for GPT response generation
# Controls the maximum length of AI responses.
#
# Higher = Longer, more detailed responses, higher API costs
# Lower = Shorter, more concise responses, lower API costs
#
# Streaming responses use a separate limit for efficiency.
MAX_TOKENS=300

# Maximum tokens for streaming GPT responses
# Typically lower than MAX_TOKENS since streaming enables earlier playback.
#
# Default 200 provides good balance between response length and latency.
MAX_TOKENS_STREAMING=200

# ============================================================================
# OPENAI MODEL CONFIGURATION
# ============================================================================
# OpenAI API models for speech recognition, conversation, and synthesis

# Whisper model for speech-to-text transcription
# Currently only "whisper-1" is available via API
# Extremely accurate for most accents and audio conditions
WHISPER_MODEL=whisper-1

# GPT model for generating conversational responses
# Options:
#   - gpt-4o-mini: Fast, cost-effective, great quality (recommended)
#   - gpt-4o: More capable but 10x more expensive
#   - gpt-3.5-turbo: Cheaper but lower quality responses
GPT_MODEL=gpt-4o-mini

# Text-to-speech model for voice synthesis
# Options:
#   - gpt-4o-mini-tts: New model with tone/style control (recommended)
#   - tts-1: Fast, lower quality, no tone control
#   - tts-1-hd: Slower, higher quality, no tone control
#
# Note: Voice, speed, and style are defined per-personality in the
# personality.yaml file, not here. The gpt-4o-mini-tts model supports
# natural language instructions for accent, tone, emotion, etc.
TTS_MODEL=gpt-4o-mini-tts

# ============================================================================
# ANIMATRONIC CONTROL SETTINGS
# ============================================================================
# Configuration for Teddy Ruxpin motor control via PPM signals
#
# The system generates stereo audio where:
#   LEFT channel  = Voice audio (what Teddy says)
#   RIGHT channel = PPM control track (pulse position modulation signals)
#
# PPM signals encode 8 channels of motor control at 60Hz frame rate:
#   Channel 1: Mouth position (syllable-based lip sync)
#   Channel 2: Eye position (sentiment-based expressions)
#   Channels 3-8: Reserved for future use (blinks, head movement, etc.)

# Audio playback preroll (milliseconds)
# Adds a small delay before audio playback starts to prevent clipping
# the first syllable. Compensates for audio buffer initialization time.
#
# Too low = First word may be cut off
# Too high = Noticeable delay before speech starts
#
# 240ms works well for most Bluetooth audio devices
PLAYBACK_PREROLL_MS=240

# Voice audio volume multiplier (0.0 to 2.0)
# Controls the loudness of the spoken voice in the LEFT channel.
#
# 1.0 = Normal volume (no change)
# >1.0 = Louder (boost volume)
# <1.0 = Quieter (reduce volume)
#
# Default 1.05 provides a slight boost for clearer playback through
# Teddy's small speaker while avoiding distortion.
VOICE_GAIN=1.05

# Control track volume multiplier (0.0 to 1.0)
# Controls the amplitude of PPM signals in the RIGHT channel.
#
# Too high = Motors may move too aggressively or cause mechanical stress
# Too low = Motors may not respond or move weakly
#
# Default 0.52 provides smooth motor movement while minimizing audio
# bleed-through from the control track into the voice channel.
CONTROL_GAIN=0.52

# Sentiment analysis thresholds for eye expressions
# The system analyzes the emotional tone of responses using VADER sentiment
# analysis and adjusts eye position accordingly:
#
#   Positive sentiment (happy/excited) = Eyes wide open
#   Neutral sentiment = Normal eye position
#   Negative sentiment (sad/concerned) = Eyes partially closed
#
# Threshold values range from -1.0 (very negative) to +1.0 (very positive)

# Positive sentiment threshold
# Response sentiment above this value triggers wide-open eyes
# Lower = Eyes open more frequently (more enthusiastic)
# Higher = Eyes only open for very positive responses
SENTIMENT_POSITIVE_THRESHOLD=0.3

# Negative sentiment threshold
# Response sentiment below this value triggers partially-closed eyes
# Higher (closer to 0) = Eyes close more frequently
# Lower (more negative) = Eyes only close for very sad responses
SENTIMENT_NEGATIVE_THRESHOLD=-0.3

# ============================================================================
# WAKE WORD DETECTION THRESHOLD
# ============================================================================
# Detection confidence threshold (0.0 to 1.0)
# Controls how confident the system must be before triggering on wake word.
#
# Higher (0.95-0.99) = Fewer false positives, may miss some valid wake words
# Lower (0.80-0.90) = More sensitive, but may trigger on similar sounds
#
# Default 0.99 is very strict and works well in most environments.
# Reduce to 0.95 if you find yourself having to repeat the wake word often.
# Increase to 0.995 in very noisy environments with frequent false triggers.
WAKE_WORD_THRESHOLD=0.99

# ============================================================================
# RVC VOICE CONVERSION (Optional)
# ============================================================================
# RVC (Retrieval-based Voice Conversion) allows you to apply custom trained
# voice models to transform the TTS output, enabling truly unique character
# voices that go beyond what OpenAI TTS can provide.
#
# RVC is configured per-personality in the personality.yaml file. This global
# setting provides an override to disable RVC entirely if needed.
#
# To use RVC for a personality:
# 1. Obtain or train an RVC model (.pth file) for the desired voice
# 2. Place the model in the personality directory or global rvc_models/ folder
# 3. Add RVC configuration to the personality's personality.yaml file
# 4. Install RVC dependencies: pip install rvc-python
#
# See docs/RVC_INTEGRATION.md for detailed setup instructions.

# Global RVC enable/disable override
# Set to false to disable RVC for all personalities (useful for debugging)
# Individual personalities can still enable/disable RVC via personality.yaml
RVC_ENABLED=true

# RVC inference device (cpu/mps/cuda)
# - cpu: Works everywhere, slower
# - mps: Apple Silicon GPU (M1/M2/M3) - 3-5x faster than CPU
# - cuda: NVIDIA GPU (if available)
# Server will auto-fallback to cpu if requested device is not available
RVC_DEVICE=mps

# Global RVC model directory
# RVC models can be placed here and shared across personalities
# Models in personality directories take precedence over global models
# RVC_MODEL_DIR=./rvc_models/

# ============================================================================
# DEBUG SETTINGS
# ============================================================================
# Development and troubleshooting configuration

# Enable debug mode for detailed logging
# When true, enables verbose logging throughout the application for
# troubleshooting issues.
DEBUG_MODE=false

# Logging level
# Controls the verbosity of log messages written to console and jf_sebastian.log
#
# Options (from most to least verbose):
#   - DEBUG: Everything including internal state changes
#   - INFO: General operational messages (recommended for normal use)
#   - WARNING: Only warnings and errors
#   - ERROR: Only error messages
LOG_LEVEL=INFO

# Save audio files for debugging
# When true, saves input and output audio files to DEBUG_AUDIO_PATH
# for inspection in audio editors like Audacity.
#
# Input files: Captured microphone audio (your voice)
# Output files: Stereo audio for your device (format depends on OUTPUT_DEVICE_TYPE)
#
# Useful for diagnosing:
#   - Microphone pickup issues
#   - VAD sensitivity problems
#   - PPM signal generation (view control track waveform)
#   - Audio quality and volume levels
SAVE_DEBUG_AUDIO=false

# Directory for saved debug audio files
# Files are named with timestamps: input_YYYYMMDD_HHMMSS.wav
DEBUG_AUDIO_PATH=./debug_audio/
