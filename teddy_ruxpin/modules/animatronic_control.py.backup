"""
Animatronic control signal generation for 1985 Teddy Ruxpin.
Generates mouth control (amplitude-based) and eye control (sentiment-based) signals.
"""

import logging
import io
import random
from typing import Optional, Tuple

import numpy as np
from scipy import signal
from scipy.io import wavfile
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer

from teddy_ruxpin.config import settings
from teddy_ruxpin.modules.ppm_generator import PPMGenerator

logger = logging.getLogger(__name__)


class AnimatronicControlGenerator:
    """
    Generates control signals for Teddy Ruxpin's motors.
    Creates stereo output: LEFT = voice audio, RIGHT = control signals.
    """

    def __init__(self):
        """Initialize control generator."""
        self.sentiment_analyzer = SentimentIntensityAnalyzer()
        # Generate PPM at 44.1kHz to avoid resampling artifacts
        self.ppm_sample_rate = 44100
        self.ppm_generator = PPMGenerator(sample_rate=self.ppm_sample_rate)
        self._last_blink_position = 0
        logger.info(f"Animatronic control generator initialized (PPM mode, {self.ppm_sample_rate}Hz)")

    def create_stereo_output(
        self,
        voice_audio_mp3: bytes,
        response_text: str
    ) -> Optional[Tuple[np.ndarray, int]]:
        """
        Create stereo audio output with voice and PPM control signals.

        Args:
            voice_audio_mp3: Voice audio data from TTS (MP3 format)
            response_text: Original text for sentiment analysis

        Returns:
            Tuple of (stereo_audio_array, sample_rate), or None on error
            stereo_audio_array shape: (num_samples, 2) where:
                - Column 0: LEFT channel (voice)
                - Column 1: RIGHT channel (PPM control signals)
        """
        try:
            # Convert MP3 to PCM audio array
            voice_audio = self._mp3_to_pcm(voice_audio_mp3)

            if voice_audio is None:
                logger.error("Failed to convert MP3 to PCM")
                return None

            # Analyze sentiment for eye control
            sentiment = self._analyze_sentiment(response_text)

            # Resample voice audio to match PPM sample rate
            # (Resample voice, NOT PPM, to preserve precise pulse timing)
            from scipy import signal as scipy_signal
            voice_duration = len(voice_audio) / settings.SAMPLE_RATE
            voice_samples_needed = int(voice_duration * self.ppm_sample_rate)
            voice_audio_resampled = scipy_signal.resample(voice_audio, voice_samples_needed)

            # Generate PPM channel values using syllable-based lip sync
            channel_values = self.ppm_generator.audio_to_channel_values(
                voice_audio,
                settings.SAMPLE_RATE,
                text=response_text,
                eyes_base=0.5,
                sentiment=sentiment
            )

            # Generate PPM control signal at 44.1kHz (no resampling needed)
            ppm_signal = self.ppm_generator.generate_ppm_signal(voice_duration, channel_values)

            # Ensure same length
            min_length = min(len(voice_audio_resampled), len(ppm_signal))
            voice_audio_resampled = voice_audio_resampled[:min_length]
            ppm_signal = ppm_signal[:min_length]

            # Apply channel-specific gains to balance audio vs control
            # Voice: boost by 25% for louder audio
            voice_gain = 0.7 * 1.25  # 87.5%
            voice_audio_resampled = voice_audio_resampled * voice_gain

            # Control: reduce by 25% to minimize bleedover
            control_gain = 0.7 * 0.75  # 52.5%
            ppm_signal = ppm_signal * control_gain

            # Create stereo array
            # LEFT=voice, RIGHT=control (original Teddy Ruxpin format)
            stereo_audio = np.column_stack((voice_audio_resampled, ppm_signal))

            logger.info(
                f"Stereo output created (PPM): {stereo_audio.shape[0]} samples at {self.ppm_sample_rate}Hz, "
                f"sentiment={sentiment:.2f}, {len(channel_values)} PPM frames, "
                f"voice_gain={voice_gain:.2f}, control_gain={control_gain:.2f}"
            )

            return stereo_audio, self.ppm_sample_rate

        except Exception as e:
            logger.error(f"Error creating stereo output: {e}", exc_info=True)
            return None

    def _mp3_to_pcm(self, mp3_data: bytes) -> Optional[np.ndarray]:
        """
        Convert MP3 audio to PCM array using FFmpeg.

        Args:
            mp3_data: MP3 audio bytes

        Returns:
            Numpy array of audio samples (normalized to -1.0 to 1.0)
        """
        try:
            import subprocess
            import tempfile

            # Write MP3 to temporary file
            with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as mp3_file:
                mp3_file.write(mp3_data)
                mp3_path = mp3_file.name

            # Use FFmpeg to convert MP3 to raw PCM
            # Output: mono, 16-bit signed, little-endian, at target sample rate
            result = subprocess.run([
                'ffmpeg',
                '-i', mp3_path,
                '-f', 's16le',  # 16-bit signed little-endian PCM
                '-acodec', 'pcm_s16le',
                '-ac', '1',  # mono
                '-ar', str(settings.SAMPLE_RATE),  # target sample rate
                '-'  # output to stdout
            ], capture_output=True, check=True)

            # Clean up temp file
            import os
            os.unlink(mp3_path)

            # Convert to numpy array
            samples = np.frombuffer(result.stdout, dtype=np.int16).astype(np.float32)

            # Normalize to -1.0 to 1.0
            samples = samples / 32768.0  # 16-bit audio max value

            return samples

        except subprocess.CalledProcessError as e:
            logger.error(f"FFmpeg error: {e.stderr.decode() if e.stderr else str(e)}")
            return None
        except Exception as e:
            logger.error(f"Error converting MP3 to PCM: {e}", exc_info=True)
            return None

    def _analyze_sentiment(self, text: str) -> float:
        """
        Analyze sentiment of text.

        Args:
            text: Text to analyze

        Returns:
            Sentiment score (-1.0 to 1.0, where positive = happy, negative = sad)
        """
        try:
            scores = self.sentiment_analyzer.polarity_scores(text)
            compound_score = scores['compound']
            logger.debug(f"Sentiment analysis: {text[:50]}... = {compound_score:.2f}")
            return compound_score
        except Exception as e:
            logger.error(f"Error analyzing sentiment: {e}")
            return 0.0  # Neutral


def save_stereo_wav(stereo_audio: np.ndarray, sample_rate: int, filename: str):
        """
        Generate mouth control signal from audio amplitude.

        Args:
            audio: Audio samples array

        Returns:
            Control signal array (same length as audio)
        """
        # Calculate RMS amplitude in sliding windows
        window_size = int(settings.SAMPLE_RATE * 0.05)  # 50ms windows
        hop_size = int(window_size / 4)  # 25% overlap

        # Calculate envelope
        amplitude = np.abs(audio)

        # Apply smoothing
        envelope = signal.savgol_filter(
            amplitude,
            window_length=min(window_size, len(amplitude) if len(amplitude) % 2 == 1 else len(amplitude) - 1),
            polyorder=3,
            mode='nearest'
        )

        # Normalize to 0-1 range
        max_amp = np.max(envelope)
        if max_amp > 0:
            envelope = envelope / max_amp
        else:
            envelope = np.zeros_like(envelope)

        # Apply smoothing factor from settings
        if settings.MOUTH_SMOOTHING > 0:
            envelope = self._apply_smoothing(envelope, settings.MOUTH_SMOOTHING)

        # Map to control signal amplitude
        # Higher audio amplitude = higher control signal amplitude = more mouth opening
        control_signal = envelope

        return control_signal

    def _generate_eye_control(self, length: int, sentiment: float) -> np.ndarray:
        """
        Generate eye control signal from sentiment.

        Args:
            length: Length of signal to generate
            sentiment: Sentiment score (-1.0 to 1.0)

        Returns:
            Control signal array
        """
        # Map sentiment to eye position
        # Positive = high amplitude (eyes up/wide)
        # Negative = low amplitude (eyes down/narrow)
        # Neutral = medium amplitude (centered)

        if sentiment > settings.SENTIMENT_POSITIVE_THRESHOLD:
            base_amplitude = 0.8  # Happy - eyes up
        elif sentiment < settings.SENTIMENT_NEGATIVE_THRESHOLD:
            base_amplitude = 0.3  # Sad - eyes down
        else:
            base_amplitude = 0.5  # Neutral - centered

        # Create sustained signal with occasional blinks
        eye_signal = np.full(length, base_amplitude, dtype=np.float32)

        # Add blinks (brief amplitude spikes)
        sample_rate = settings.SAMPLE_RATE
        blink_duration = int(sample_rate * 0.15)  # 150ms blink

        # Calculate blink times
        duration_seconds = length / sample_rate
        avg_blink_interval = (settings.EYE_BLINK_MIN + settings.EYE_BLINK_MAX) / 2
        num_blinks = int(duration_seconds / avg_blink_interval)

        for _ in range(num_blinks):
            # Random blink timing
            blink_time = random.uniform(
                settings.EYE_BLINK_MIN,
                min(settings.EYE_BLINK_MAX, duration_seconds)
            )
            blink_position = int(blink_time * sample_rate)

            # Ensure within bounds
            if blink_position + blink_duration < length:
                # Create blink pulse (brief drop in amplitude)
                blink_start = blink_position
                blink_end = min(blink_position + blink_duration, length)

                # Smooth blink transition
                blink_curve = np.sin(np.linspace(0, np.pi, blink_end - blink_start))
                eye_signal[blink_start:blink_end] = base_amplitude * (1 - 0.5 * blink_curve)

        return eye_signal

    def _combine_control_signals(
        self,
        mouth_signal: np.ndarray,
        eye_signal: np.ndarray
    ) -> np.ndarray:
        """
        Combine mouth and eye control signals using carrier frequency modulation.

        The 1985 Teddy Ruxpin mechanism uses a carrier tone (60-80 Hz) that's
        amplitude-modulated to control motor positions. Higher amplitude = more movement.

        Args:
            mouth_signal: Mouth control signal (0-1 range)
            eye_signal: Eye control signal (0-1 range)

        Returns:
            Combined control signal with carrier tone modulation
        """
        # Ensure same length
        min_length = min(len(mouth_signal), len(eye_signal))
        mouth_signal = mouth_signal[:min_length]
        eye_signal = eye_signal[:min_length]

        # Combine control signals (mouth is primary, eyes secondary)
        # Weight mouth more heavily as it's the primary movement
        combined_envelope = 0.7 * mouth_signal + 0.3 * eye_signal

        # Generate carrier frequency from settings
        carrier_freq = settings.CONTROL_CARRIER_FREQ
        sample_rate = settings.SAMPLE_RATE
        t = np.arange(len(combined_envelope)) / sample_rate

        # Create carrier wave (sine wave)
        carrier = np.sin(2 * np.pi * carrier_freq * t)

        # Amplitude modulate the carrier with the control envelope
        # Map envelope from 0-1 to configured amplitude range
        amplitude_range = settings.CONTROL_MAX_AMPLITUDE - settings.CONTROL_MIN_AMPLITUDE
        modulated_envelope = settings.CONTROL_MIN_AMPLITUDE + amplitude_range * combined_envelope

        # Apply modulation
        modulated_signal = carrier * modulated_envelope

        # The signal is now in -1 to 1 range, which is perfect for audio output
        logger.debug(f"Control signal generated: carrier={carrier_freq}Hz, "
                    f"amplitude range={settings.CONTROL_MIN_AMPLITUDE:.2f}-{settings.CONTROL_MAX_AMPLITUDE:.2f}")

        return modulated_signal

    def _apply_smoothing(self, signal_data: np.ndarray, smoothing_factor: float) -> np.ndarray:
        """
        Apply exponential moving average smoothing.

        Args:
            signal_data: Input signal
            smoothing_factor: Smoothing factor (0 = no smoothing, 1 = maximum smoothing)

        Returns:
            Smoothed signal
        """
        if smoothing_factor <= 0:
            return signal_data

        smoothed = np.zeros_like(signal_data)
        smoothed[0] = signal_data[0]

        alpha = 1 - smoothing_factor

        for i in range(1, len(signal_data)):
            smoothed[i] = alpha * signal_data[i] + (1 - alpha) * smoothed[i - 1]

        return smoothed


def save_stereo_wav(stereo_audio: np.ndarray, sample_rate: int, filename: str):
    """
    Save stereo audio to WAV file for debugging.

    Args:
        stereo_audio: Stereo audio array (num_samples, 2)
        sample_rate: Sample rate in Hz
        filename: Output filename
    """
    try:
        # Convert to int16 for WAV
        audio_int16 = (stereo_audio * 32767).astype(np.int16)

        wavfile.write(filename, sample_rate, audio_int16)
        logger.info(f"Stereo audio saved to {filename}")
    except Exception as e:
        logger.error(f"Error saving stereo audio: {e}", exc_info=True)
